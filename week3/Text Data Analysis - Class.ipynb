{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e5faf5-fd1a-450b-a3e4-81b005263593",
   "metadata": {},
   "source": [
    "# Text Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6911bf9-9598-466a-9fe0-7c1a26c96ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import flair\n",
    "import gensim\n",
    "import umap\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8c4c2-be97-412f-9ecd-64652e3f4e00",
   "metadata": {},
   "source": [
    "## 1. Hansard Data\n",
    "\n",
    "In this section, we explore Hansard data, which consists of speeches and debates made in Singapore's Parliament Chamber and provides a record of parliamentary business and proceedings in a Sitting. Data from Hansard has already been scraped for you, focusing specifically on the Committee of Supply (or Budget) debates for the 15th Parliament (from 2021 to present). We will use this as an opportunity to explore sentiment analysis and topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0084219-821a-458e-ac35-b98d09e55347",
   "metadata": {},
   "source": [
    "### 1.1 Importing the data and doing simple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6298ec-66c3-43ad-b1d2-89ffaca14cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Hansard data\n",
    "hansard_df = pd.read_csv(\"Hansard_15th_Parl_COS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48c668-282a-4d24-88a0-86e1296e15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hansard_df.shape)\n",
    "hansard_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e73e1c-184d-409b-9437-5a3e42a245ab",
   "metadata": {},
   "source": [
    "Let's try to enrich this dataset with some useful variables. \n",
    "\n",
    "<span style=\"background-color: #FFFF00\">**Exercise:** Create two new columns for this dataset: \n",
    "* `Sitting Year` (int): Year in which the speech was given\n",
    "* `Speech Length` (int): Number of words in the speech </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f31692-fc46-4e05-a2d0-729b46529b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bed0b-2468-4abd-acb4-d605c1e62819",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0438f-cccf-43c4-a53a-394890da4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df['Speech Length'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6d5f0-b140-406c-87b0-049ad9a77bfa",
   "metadata": {},
   "source": [
    "### 1.2 Sentiment Analysis\n",
    "\n",
    "Let's start with applying some sentiment analysis. While most Parliamentary speeches are likely to be quite mild in terms of sentiment, we might be able to identify some more impassioned speeches. Before you proceed, make sure you have both the `spacy` and `spacytextblob` libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42817352-c78c-411c-a495-c17f73e83e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command here to download textblob's additional corpuses\n",
    "!python -m textblob.download_corpora\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381cd9c-fdb7-4ef3-8b7b-7ab3fe5aa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Initialise the NLP pipeline and add the spacetextblob step to the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "# Add our texts\n",
    "texts = hansard_df['Speech']\n",
    "\n",
    "# This will take about 20-30 seconds to run\n",
    "sentiment_results = []\n",
    "for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"ner\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    sentiment_results.append({\n",
    "        'Polarity': doc._.blob.polarity,\n",
    "        'Subjectivity': doc._.blob.subjectivity, \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc179a-6546-4047-a11a-c6998bd2f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we append it to our original dataset\n",
    "sentiment_results_df = pd.DataFrame(sentiment_results)\n",
    "hansard_df = pd.concat([hansard_df, sentiment_results_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192ec0d-fb6c-43ef-824b-648493d07137",
   "metadata": {},
   "source": [
    "Let's do some simple data analysis to understand the distributions of the polarity and subjectivity scores. Before you run the cells below, think carefully about what you expect from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a9f74-2ae8-4f51-a7a7-50df4bcac3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity scores range from [-1 to 1], with -1 indicating very negative and 1 indicating very positive\n",
    "hansard_df['Polarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae48303-fe16-4db6-9af9-b054374c5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity scores range from [0 to 1], with 0 indicating very objective and 1 indicating very subjective\n",
    "hansard_df['Subjectivity'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0805e-0674-40c6-8839-7b28e3bc0fb9",
   "metadata": {},
   "source": [
    "Unsurprisingly most texts are neutral and objective. But this may be swayed by the number of times the Chairman speaks. Let's filter that out and look at this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d788b-ef87-4a64-aec6-24b0b83bfb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned = hansard_df[hansard_df['Speaker'] != \"The Chairman\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30def40e-b6e6-4864-9488-b2cb06d94913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity scores range from [-1 to 1], with -1 indicating very negative and 1 indicating very positive\n",
    "hansard_df_cleaned['Polarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555e1ab-056b-4b88-b153-3b414b8fbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity scores range from [0 to 1], with 0 indicating very objective and 1 indicating very subjective\n",
    "hansard_df_cleaned['Subjectivity'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbf6b2-bd63-4780-addc-218e7418a8be",
   "metadata": {},
   "source": [
    "Let's find the most positive speech and the most negative speech! Share your thoughts about why you think these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18079f6b-2111-4204-92cb-e6714f9cf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.loc[hansard_df_cleaned['Polarity'].argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d564699-1ef8-42eb-8685-c856fa61ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.loc[hansard_df_cleaned['Polarity'].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ff8e5-9f55-4f66-a9d2-8fb72932b7fb",
   "metadata": {},
   "source": [
    "Both of these speeches seem a bit short, which might explain their extreme polarity scores. Let's plot a scatter plot to highlight the relationship between speech length and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999be57-72f2-48d4-9c70-086935a3314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.plot.scatter(x = 'Speech Length', y = 'Polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98306807-f285-47f6-ba9b-43cc225c9405",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">**Class Discussion:** Given that `textblob` is a dictionary-based approach to sentiment analysis, can you think of why longer speeches tend to have less extreme values for positive/negative sentiment?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f7ce8-0e51-4d3f-acd9-ed5871362d3e",
   "metadata": {},
   "source": [
    "Now let's try a different approach: using an embedding-based classifier instead! We will use a small embedding-based classifier that has already been finetuned to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423ba3a-fb13-4002-b256-7891a660e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "tagger = Classifier.load('./flair_sentiment.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363c0e2-ab5c-4857-b2e1-e35e475ac266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it out with a random speech\n",
    "sentence = Sentence(hansard_df_cleaned['Speech'][2])\n",
    "tagger.predict(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff1ee5-fdac-4661-9515-24292a166325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = []\n",
    "\n",
    "# This should take around 3-5 minutes\n",
    "for text in tqdm(hansard_df_cleaned['Speech'].tolist()):\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    # Remember to take the inverse of the negative score\n",
    "    if sentence.labels[0].value == 'NEGATIVE':\n",
    "        sentiment_scores.append(1 - sentence.labels[0].score)\n",
    "    else:\n",
    "        sentiment_scores.append(sentence.labels[0].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053f049-2e44-426b-b37e-79f42e0f7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned['Sentiment'] = sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70440f86-23fa-4fc8-ae70-e2bfa8c9dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned['Sentiment'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2f49f-39c7-402d-aa94-6e248ab60c8b",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">**Class Discussion:** What do you notice about this chart that is different from the `textblob` model results? Why do you think there is such a big difference?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed5275-df94-4cd5-b6d3-1f21c637d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_df_cleaned.plot.scatter(x = 'Speech Length', y = 'Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113659c6-b7f2-4d40-9fb5-47c2666ad090",
   "metadata": {},
   "source": [
    "### 1.3 Topic modelling\n",
    "\n",
    "Since Parliamentary debates tend to be quite topic-focused, topic modelling would be a good option for us to better understand the ongoing debates and to get a sense of the priority areas for discussion in Parliament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32a912-00e8-46a5-8df6-b6942e30a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['also', 'mr', 'chairman', 'beg', 'move'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Stopword removal and lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf066531-9cac-4007-84d2-98ee2ea2be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Preprocessing (with stopword removal and lemmatization)\n",
    "texts_preprocessed = [preprocess(text) for text in hansard_df_cleaned['Speech']]\n",
    "\n",
    "# Step 2: Vectorizing the text data\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(texts_preprocessed)\n",
    "\n",
    "# Step 3: Applying LDA for Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 2024)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Step 4: Extracting and Displaying Topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bce67-7b58-400a-b9e2-119d4bed8cbc",
   "metadata": {},
   "source": [
    "The topics looks fairly sensible, but is there a way for us to get a more tangible and concrete way to assess the quality of this topic modelling? We can look at the **coherence score** for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321fda6-b480-4a88-9567-b95fb78765e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Step 1: Create a Gensim Dictionary and Corpus\n",
    "texts_tokenized = [text.split() for text in texts_preprocessed]\n",
    "dictionary = Dictionary(texts_tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n",
    "\n",
    "# Step 2: Get the topics from the LDA model\n",
    "lda_topics = lda.components_\n",
    "lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]\n",
    "\n",
    "# Step 3: Calculate Coherence Score\n",
    "coherence_model_lda = CoherenceModel(topics = lda_topics_words, \n",
    "                                     texts = texts_tokenized, \n",
    "                                     dictionary = dictionary, \n",
    "                                     coherence = 'c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score for LDA Model: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4d11b-1e3e-49e8-a31d-b6b62602edf3",
   "metadata": {},
   "source": [
    "Now let's try varying some of the parameters to see which gets us the optimal coherence score. We'll start by adjusting the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871df834-0e55-49dc-ae54-3fc11fb18cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_list = [3, 5, 10, 15, 20, 25]\n",
    "coherence_scores = []\n",
    "\n",
    "# It should take around 15-30 seconds for each iteration\n",
    "for n_topics in tqdm(n_topics_list):\n",
    "        \n",
    "    lda = LatentDirichletAllocation(n_components = n_topics, random_state = 2024)\n",
    "    lda.fit(dtm)\n",
    "    lda_topics = lda.components_\n",
    "    lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]    \n",
    "    coherence_model_lda = CoherenceModel(topics = lda_topics_words, \n",
    "                                         texts = texts_tokenized, \n",
    "                                         dictionary = dictionary, \n",
    "                                         coherence = 'c_v')    \n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"Number of topics: {n_topics} | Coherence Score: {coherence_lda}\")\n",
    "    coherence_scores.append(coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4743ec4-dae8-446d-a756-4f884920af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(n_topics_list, coherence_scores)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486236eb-5fb3-486a-b0e4-68e3d55ea1cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e977d-8fdc-40de-8270-bb8c852f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 25, random_state = 2024)\n",
    "lda.fit(dtm)\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4421fd-e841-4a45-b539-d0bf0ae4da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the topic distribution for each document\n",
    "lda_topic_distributions = lda.transform(dtm)\n",
    "\n",
    "# Step 2: Identify the dominant topic for each document\n",
    "dominant_topics = np.argmax(lda_topic_distributions, axis = 1)\n",
    "\n",
    "# Step 3: Apply UMAP to reduce to 2 dimensions\n",
    "umap_model = umap.UMAP(n_components = 2, random_state = 2024)\n",
    "lda_2d = umap_model.fit_transform(lda_topic_distributions)\n",
    "\n",
    "# Step 4: Prepare data for Plotly\n",
    "df = pd.DataFrame({\n",
    "    'UMAP1': lda_2d[:, 0],\n",
    "    'UMAP2': lda_2d[:, 1],\n",
    "    'Dominant Topic': dominant_topics,\n",
    "    'Text': hansard_df_cleaned['Speech'].str.slice(0,1000).tolist()  \n",
    "})\n",
    "\n",
    "# Step 5: Create custom hover template to control text width\n",
    "hover_template = '<br>'.join(['%{customdata}'])\n",
    "\n",
    "# Limiting the line width by adding line breaks after a specific number of characters (e.g., 50)\n",
    "df['Text'] = df['Text'].apply(lambda x: '<br>'.join([x[i:i+50] for i in range(0, len(x), 50)]))\n",
    "\n",
    "# Step 6: Create an interactive plot with Plotly\n",
    "fig = px.scatter(\n",
    "    df, x='UMAP1', y='UMAP2',\n",
    "    color='Dominant Topic',\n",
    "    custom_data=['Text'],  # Use custom data for hover template\n",
    "    title='Interactive UMAP Projection of LDA Topic Distributions',\n",
    "    color_continuous_scale=px.colors.qualitative.Set1\n",
    ")\n",
    "\n",
    "# Customize hover template to use our custom text formatting\n",
    "fig.update_traces(\n",
    "    hovertemplate=hover_template,\n",
    "    marker=dict(size=8, opacity=0.7)\n",
    ")\n",
    "\n",
    "# Customize layout with specific dimensions\n",
    "fig.update_layout(\n",
    "    width = 1200,\n",
    "    height = 800,\n",
    "    legend_title_text='Dominant Topic',\n",
    "    legend = dict(\n",
    "        itemsizing='constant'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d82a23-325d-4c20-81a0-4146e49edeac",
   "metadata": {},
   "source": [
    "Now we try with another topic modelling approach: using embeddings with BERTopic. Note that BERTopic relies on hierarchical clustering, so we don't have to set any number of topics as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db7579-c920-4db4-8a9e-f11b58bfe25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Step 1: Initialize BERTopic\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Step 2: Fit the model to your data\n",
    "topics, probabilities = topic_model.fit_transform(hansard_df_cleaned['Speech'].tolist())\n",
    "\n",
    "# Step 3: View the topics\n",
    "topics_overview = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d621c-ce2d-48d0-a103-49fc7f236ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c9625-cff4-40b2-9521-02920d67288d",
   "metadata": {},
   "source": [
    "BERTopic says we have 74 topics, which sounds like a lot of topics compared to what we had previously! Unfortunately it also seems like 604 (or about 17% of the data) are considered as \"outliers\". Let's use some of the data visualisation tools to get a visual appreciation of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed95cfa-3b85-42cd-820e-2152c6efa323",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be955b6-2bd1-4165-8df6-86836ae0e8e8",
   "metadata": {},
   "source": [
    "It seems like some of these clusters are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0dd0a-efe3-47f0-ae7e-2f9687710bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(hansard_df_cleaned['Speech'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e1f11-f169-4d96-b703-97172e1b375c",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">**Class Discussion:** What are your observations about the quality of the topics identified here, versus the topics identified by the LDA model? Are there significant differences, and if so, in what ways?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f8eff-ed26-42cc-b281-9fc6e64d1e17",
   "metadata": {},
   "source": [
    "## 2. NUS SMS Data\n",
    "\n",
    "In this section we explore the NUS SMS corpus that was released [here](https://github.com/kite1988/nus-sms-corpus), mainly to demonstrate the challenges of analysing Singlish data and how conventional NLP techniques may fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac36f9-03c6-4d73-b927-a3f4f357d38d",
   "metadata": {},
   "source": [
    "### 2.1 Importing the data and doing simple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418538e1-ad13-499a-bf28-eb6762dae27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"smsCorpus_en_2015.03.09_all.json\", 'r') as file:\n",
    "    sms_corpus = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb6394-c19e-4797-96e5-624026d08e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many messages there are in this corpus\n",
    "len(sms_corpus['smsCorpus']['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4753b3-92d8-43ae-89d4-aaf764f030ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first message\n",
    "sms_corpus['smsCorpus']['message'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d01ce-43ef-4a1d-ac86-9bc787606b58",
   "metadata": {},
   "source": [
    "Now we write a function to extract all the SMSes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e5412-d458-4661-8758-dc7172f0877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_list = []\n",
    "for message in sms_corpus['smsCorpus']['message']:\n",
    "    sms_corpus_list.append({\n",
    "        'ID': message['@id'],\n",
    "        'Text': message['text']['$']\n",
    "    })\n",
    "sms_corpus_df = pd.DataFrame(sms_corpus_list)\n",
    "sms_corpus_df['Text'] = sms_corpus_df['Text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c8971-c075-4034-a6f5-a60e279e9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cacdea-8de5-42c8-8861-781007629025",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">**Exercise:** Create two new columns for this dataset:  </span>\n",
    "* Word Count (int): How many words are in the text\n",
    "* Polarity (float): How positive or negative the text is (using `textblob` and `spacy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a5566-64de-4259-acd1-f98dbb77ec8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0ace2-f94f-4fa4-8bdd-5bf822f586dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_df['Word Count'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f65f0d-079e-47a2-9071-35d024daedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_df['Polarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4ffa8-9f2f-46a7-909c-eb5db17ba3dd",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">**Class Discussion:** Before you ran these plots, what were you expecting? Now after having seen these plots, what are your thoughts? Is this what you had expected, and why?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d5c71-8b19-4204-9f0a-412e293a25b5",
   "metadata": {},
   "source": [
    "### 2.2: Topic modelling\n",
    "\n",
    "We try with some topic modelling to highlight the challenges of topic modelling with short texts, on top of the difficulties with Singlish texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a661e6-f6af-4d5e-a561-8a3748b7b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing (with stopword removal and lemmatization)\n",
    "texts_preprocessed = [preprocess(text) for text in sms_corpus_df['Text'].astype('str')]\n",
    "\n",
    "# Step 2: Vectorizing the text data\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(texts_preprocessed)\n",
    "\n",
    "# Step 3: Applying LDA for Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 2024)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Step 4: Extracting and Displaying Topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed1691-c2c3-4d07-ad3e-7ec17b660c39",
   "metadata": {},
   "source": [
    "The topics here look quite bad, but this is unsurprising given how short SMSes are. Topic modelling tends to underperform in these cases. We check this by computing the coherence score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883f91c-eede-482c-8221-ccf4196c9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Gensim Dictionary and Corpus\n",
    "texts_tokenized = [text.split() for text in texts_preprocessed]\n",
    "dictionary = Dictionary(texts_tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n",
    "\n",
    "# Step 2: Get the topics from the LDA model\n",
    "lda_topics = lda.components_\n",
    "lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]\n",
    "\n",
    "# Step 3: Calculate Coherence Score\n",
    "coherence_model_lda = CoherenceModel(topics = lda_topics_words, \n",
    "                                     texts = texts_tokenized, \n",
    "                                     dictionary = dictionary, \n",
    "                                     coherence = 'c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score for LDA Model: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb823f-1533-4235-8adc-b8e23133df62",
   "metadata": {},
   "source": [
    "One problem with Singlish is the difficulty in tokenising it correctly. Let's take a look by applying BERT's tokeniser to some of the Singlish texts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b793e-17ae-4521-9000-d41b38610b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a5877-7401-4fe0-959d-5f13a8ec42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_corpus_df['Text'][0])\n",
    "tokenizer.tokenize(sms_corpus_df['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd872c2d-54dd-4191-829b-e85ccc1800fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_corpus_df['Text'][36])\n",
    "tokenizer.tokenize(sms_corpus_df['Text'][36])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
